{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.22.4 (from pandas)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.0/61.0 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hongk\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hongk\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.7/11.6 MB 21.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 43.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.0/11.6 MB 63.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 65.1 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.7/15.8 MB 84.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.5/15.8 MB 83.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.7/15.8 MB 81.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.2/15.8 MB 93.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 65.1 MB/s eta 0:00:00\n",
      "Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "   ---------------------------------------- 0.0/505.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 505.5/505.5 kB 33.0 MB/s eta 0:00:00\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "   ---------------------------------------- 0.0/345.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 345.4/345.4 kB 20.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.26.4 pandas-2.2.2 pytz-2024.1 tzdata-2024.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install sentence-transformers\n",
    "!pip install fairseq\n",
    "!pip install langchain\n",
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp39-cp39-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hongk\\desktop\\embedding_test\\.conda\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Downloading faiss_cpu-1.8.0-cp39-cp39-win_amd64.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/14.5 MB 1.7 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 3.1/14.5 MB 40.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.3/14.5 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.4/14.5 MB 93.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.5/14.5 MB 108.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 72.5 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vector spacem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_of_documents = pd.read_excel(r\"C:\\Users\\hongk\\Desktop\\embedding_test\\vector_database.xlsx\")\n",
    "df_testing = pd.read_excel(r\"C:\\Users\\hongk\\Desktop\\embedding_test\\testing.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cardiac Catheterization',\n",
       " 'Defibrillator Implantation',\n",
       " 'Pacemaker Installation',\n",
       " 'Cardioversion',\n",
       " 'Heart Valve Surgery',\n",
       " 'Coronary Artery Bypass Grafting (CABG)',\n",
       " 'Endovenous Laser Therapy (EVLT)',\n",
       " 'Varicose Vein Treatment',\n",
       " 'Sclerotherapy',\n",
       " 'Hemorrhoidectomy',\n",
       " 'Anal Fistula Repair',\n",
       " 'Colostomy',\n",
       " 'Ileostomy',\n",
       " 'Gastrostomy',\n",
       " 'Laparoscopic Appendectomy',\n",
       " 'Cholecystectomy',\n",
       " 'Hernia Repair (Laparoscopic)',\n",
       " 'Hernia Repair (Open)',\n",
       " 'Bariatric Surgery (Gastric Bypass)',\n",
       " 'Bariatric Surgery (Gastric Sleeve)',\n",
       " 'Bariatric Surgery (LAP-BAND)',\n",
       " 'Thyroidectomy',\n",
       " 'Parathyroidectomy',\n",
       " 'Adrenalectomy',\n",
       " 'Pituitary Surgery',\n",
       " 'Brain Tumor Removal',\n",
       " 'Craniotomy',\n",
       " 'Deep Brain Stimulation (DBS)',\n",
       " 'Spinal Cord Stimulator Implantation',\n",
       " 'Hydrocephalus Shunt Placement',\n",
       " 'Epilepsy Surgery',\n",
       " 'Neuropsychological Testing',\n",
       " 'Cognitive Behavioral Therapy (CBT)',\n",
       " 'Dialectical Behavior Therapy (DBT)',\n",
       " 'Electroconvulsive Therapy (ECT)',\n",
       " 'Transcranial Magnetic Stimulation (TMS)',\n",
       " 'Phototherapy',\n",
       " 'Dermabrasion',\n",
       " 'Microdermabrasion',\n",
       " 'Chemical Peel',\n",
       " 'Mole Removal',\n",
       " 'Skin Tag Removal',\n",
       " 'Laser Hair Removal',\n",
       " 'Tattoo Removal',\n",
       " 'Scar Revision Surgery',\n",
       " 'Burn Treatment',\n",
       " 'Wart Treatment',\n",
       " 'Psoriasis Treatment',\n",
       " 'Eczema Treatment',\n",
       " 'Rosacea Treatment',\n",
       " 'Alopecia Treatment',\n",
       " 'Vitiligo Treatment',\n",
       " 'Actinic Keratosis Treatment',\n",
       " 'Basal Cell Carcinoma Treatment',\n",
       " 'Squamous Cell Carcinoma Treatment',\n",
       " 'Melanoma Treatment',\n",
       " 'Liposuction',\n",
       " 'Tummy Tuck (Abdominoplasty)',\n",
       " 'Breast Augmentation',\n",
       " 'Breast Reduction',\n",
       " 'Breast Reconstruction',\n",
       " 'Mastectomy',\n",
       " 'Gynecomastia Surgery',\n",
       " 'Facelift (Rhytidectomy)',\n",
       " 'Brow Lift',\n",
       " 'Eyelid Surgery (Blepharoplasty)',\n",
       " 'Rhinoplasty (Nasal Surgery)',\n",
       " 'Otoplasty (Ear Surgery)',\n",
       " 'Chin Augmentation',\n",
       " 'Cheek Augmentation',\n",
       " 'Jaw Surgery (Orthognathic Surgery)',\n",
       " 'Cleft Lip and Palate Surgery',\n",
       " 'Maxillofacial Surgery',\n",
       " 'Dental Implants',\n",
       " 'Root Canal Treatment',\n",
       " 'Tooth Extraction',\n",
       " 'Wisdom Tooth Removal',\n",
       " 'Teeth Whitening',\n",
       " 'Dental Braces',\n",
       " 'Invisalign Treatment',\n",
       " 'Dental Crown Placement',\n",
       " 'Dental Bridge Placement',\n",
       " 'Dental Veneers',\n",
       " 'Gum Surgery',\n",
       " 'Periodontal Treatment',\n",
       " 'Dental Filling',\n",
       " 'Fluoride Treatment',\n",
       " 'Dental Sealants',\n",
       " 'Oral Cancer Screening',\n",
       " 'Sleep Apnea Oral Appliance',\n",
       " 'TMJ Treatment',\n",
       " 'Orthodontic Consultation',\n",
       " 'Prosthodontic Consultation',\n",
       " 'Endodontic Consultation',\n",
       " 'Pedodontic Consultation',\n",
       " 'Geriatric Dentistry Consultation',\n",
       " 'Hospital Bed Rental',\n",
       " 'Home Oxygen Therapy',\n",
       " 'Wheelchair Rental',\n",
       " 'Mobility Scooter Rental',\n",
       " 'Single Room',\n",
       " 'Shared Room',\n",
       " 'Isolation Room',\n",
       " 'Recovery Room',\n",
       " 'Operating Room',\n",
       " 'Labor and Delivery Room',\n",
       " 'Post-Anesthesia Care Unit (PACU)',\n",
       " 'Burn Unit',\n",
       " 'Cardiac Care Unit (CCU)',\n",
       " 'Neurological Intensive Care Unit (NICU)',\n",
       " nan,\n",
       " 'Medical Services',\n",
       " nan,\n",
       " 'Consultation Fee (Oncologist)',\n",
       " 'Consultation Fee (Cardiologist)',\n",
       " 'Consultation Fee (Gastroenterologist)',\n",
       " 'Consultation Fee (Neurologist)',\n",
       " 'Telehealth Consultation',\n",
       " 'Virtual Consultation',\n",
       " 'Pre-Operative Assessment',\n",
       " 'Post-Operative Care',\n",
       " 'Wound Care',\n",
       " 'Pain Management',\n",
       " nan,\n",
       " 'Diagnostic Tests',\n",
       " nan,\n",
       " 'Electroencephalogram (EEG)',\n",
       " 'Holter Monitor',\n",
       " 'Stress Test',\n",
       " 'Colonoscopy',\n",
       " 'Gastroscopy',\n",
       " 'Bronchoscopy',\n",
       " 'Pap Smear',\n",
       " 'Mammogram',\n",
       " 'Bone Marrow Biopsy',\n",
       " 'Genetic Testing',\n",
       " 'Therapeutic Services',\n",
       " 'Physical Therapy',\n",
       " 'Rehabilitation Therapy',\n",
       " 'Speech and Language Therapy',\n",
       " 'Art Therapy',\n",
       " 'Music Therapy',\n",
       " 'Massage Therapy',\n",
       " 'Dialysis',\n",
       " 'Radiation Therapy',\n",
       " 'Chemotherapy',\n",
       " 'Immunotherapy',\n",
       " 'Prenatal Ultrasound',\n",
       " 'Fetal Monitoring',\n",
       " 'Labor Induction',\n",
       " 'Episiotomy Repair',\n",
       " 'Postpartum Depression Screening',\n",
       " 'Lactation Consultation',\n",
       " 'Well-Baby Check-ups',\n",
       " 'Immunizations',\n",
       " 'Early Intervention Services',\n",
       " 'Childbirth Education Classes',\n",
       " nan,\n",
       " 'Medication and Pharmacy',\n",
       " nan,\n",
       " 'Prescription Refills',\n",
       " 'Medication Administration',\n",
       " 'IV Fluid Therapy',\n",
       " 'Antibiotics',\n",
       " 'Pain Relievers',\n",
       " 'Anti-Inflammatory Medications',\n",
       " 'Anti-Depressants',\n",
       " 'Anti-Anxiety Medications',\n",
       " 'Hormone Replacement Therapy',\n",
       " 'Allergy Medications',\n",
       " nan,\n",
       " 'Laboratory Services',\n",
       " nan,\n",
       " 'Complete Blood Count (CBC)',\n",
       " 'Thyroid Function Test',\n",
       " 'Hormone Levels',\n",
       " 'Drug Screening',\n",
       " 'Tissue Culture',\n",
       " 'Serology Tests',\n",
       " 'Microbiology Tests',\n",
       " 'Parasitology Tests',\n",
       " 'Cytology Tests',\n",
       " 'Molecular Diagnostics',\n",
       " nan,\n",
       " 'Nursing Services',\n",
       " nan,\n",
       " 'Wound Dressing Changes',\n",
       " 'Catheter Care',\n",
       " 'Medication Administration',\n",
       " 'Vital Sign Monitoring',\n",
       " 'Patient Education',\n",
       " 'Discharge Planning',\n",
       " 'Palliative Care',\n",
       " 'Hospice Care',\n",
       " 'End-of-Life Care',\n",
       " 'Support Groups',\n",
       " nan,\n",
       " 'Miscellaneous Charges',\n",
       " nan,\n",
       " 'Parking Fee',\n",
       " 'Room Service Fee',\n",
       " 'Gift Shop Purchases',\n",
       " 'Telecommunication Charges',\n",
       " 'Insurance Co-payment',\n",
       " 'Out-of-Pocket Expenses',\n",
       " 'Patient Transportation',\n",
       " 'Interpreter Services',\n",
       " 'Legal Services',\n",
       " 'Social Work Services',\n",
       " nan,\n",
       " 'Specialist Services',\n",
       " nan,\n",
       " 'Cardiology',\n",
       " 'Neurology',\n",
       " 'Oncology',\n",
       " 'Gastroenterology',\n",
       " 'Urology',\n",
       " 'Gynecology',\n",
       " 'Endocrinology',\n",
       " 'Pulmonology',\n",
       " 'Rheumatology',\n",
       " 'Infectious Disease',\n",
       " 'Dental Services',\n",
       " nan,\n",
       " 'Dental Crown',\n",
       " 'Dental Bridge',\n",
       " 'Dental Implant',\n",
       " 'Dental Veneers',\n",
       " 'Root Canal Therapy',\n",
       " 'Gum Grafting',\n",
       " 'Dental Sealants',\n",
       " 'Fluoride Treatment',\n",
       " 'Mouthguard',\n",
       " 'Night Guard',\n",
       " 'Surgical Procedures',\n",
       " nan,\n",
       " 'Tonsillectomy',\n",
       " 'Appendectomy',\n",
       " 'Hysterectomy',\n",
       " 'Cataract Surgery',\n",
       " 'Knee Replacement',\n",
       " 'Hip Replacement',\n",
       " 'Spine Surgery',\n",
       " 'Bariatric Surgery',\n",
       " 'Cosmetic Surgery',\n",
       " 'Reconstructive Surgery',\n",
       " nan,\n",
       " 'Outpatient Services',\n",
       " nan,\n",
       " 'Outpatient Pharmacy',\n",
       " 'Outpatient Laboratory',\n",
       " 'Outpatient Imaging',\n",
       " 'Outpatient Rehabilitation',\n",
       " 'Outpatient Mental Health Services',\n",
       " 'Outpatient Cancer Care',\n",
       " 'Outpatient Wound Care',\n",
       " 'Outpatient Dialysis',\n",
       " 'Outpatient Infusion Therapy',\n",
       " 'Outpatient Surgery']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_testing['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Consultation Fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>General Practitioner Fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Specialist Consultation Fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Emergency Room Fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hospital Admission Fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Copy of Medical Records</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Insurance Pre-Authorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Billing Inquiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>Patient Education Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>Health Coaching Session</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           words\n",
       "0               Consultation Fee\n",
       "1       General Practitioner Fee\n",
       "2    Specialist Consultation Fee\n",
       "3             Emergency Room Fee\n",
       "4         Hospital Admission Fee\n",
       "..                           ...\n",
       "506      Copy of Medical Records\n",
       "507  Insurance Pre-Authorization\n",
       "508              Billing Inquiry\n",
       "509  Patient Education Materials\n",
       "510      Health Coaching Session\n",
       "\n",
       "[511 rows x 1 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list_of_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 15:20:44 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu\n",
      "2024-06-13 15:20:44 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: jinaai/jina-embeddings-v2-base-en\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jina-embeddings-v2-base-en\n"
     ]
    }
   ],
   "source": [
    "def remove_words_before_slash(text):\n",
    "  \"\"\"Removes all words before the first dot (.) in a string.\n",
    "\n",
    "  Args:\n",
    "    text: The input string.\n",
    "\n",
    "  Returns:\n",
    "    The string with all words before the first dot removed.\n",
    "  \"\"\"\n",
    "  parts = text.split('/')\n",
    "  if len(parts) > 1:\n",
    "    return parts[1].strip()\n",
    "  else:\n",
    "    return ''\n",
    "\n",
    "# Example usage\n",
    "text = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "result = remove_words_before_slash(text)\n",
    "print(result)  # Output: jina-embeddings-v2-base-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Read words from the CSV file\n",
    "csv_file = r\"C:\\Users\\hongk\\Desktop\\embedding_test\\vector_database.xlsx\" # update with your CSV file path\n",
    "df_list_of_documents = pd.read_excel(csv_file)\n",
    "words = df_list_of_documents['words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_db_creation(model_name,xlsx_file):\n",
    "    # time how long it takes to create the index\n",
    "    import time\n",
    "    start = time.time()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    df_list_of_documents = pd.read_excel(xlsx_file)\n",
    "    words = df_list_of_documents['words'].tolist()\n",
    "    # # Step 3: Generate vectors for the words\n",
    "    vectors = generate_embeddings(words, tokenizer, model)\n",
    "\n",
    "    # # Step 4: Create the FAISS index\n",
    "    dimension = vectors.shape[1]  # dimension of vectors (determined by the model)\n",
    "    index = faiss.IndexFlatIP(dimension)  \n",
    "    index.add(vectors)\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "    print(f\"Time taken to create the index: {end - start:.2f} seconds\")\n",
    "    df_vector_storage_time_helper = pd.DataFrame(columns = ['model_name','total_time'])\n",
    "    df_vector_storage_time_helper['model_name'] = [model_name]\n",
    "    df_vector_storage_time_helper['total_time'] = [total_time]\n",
    "    return index,tokenizer,model,df_vector_storage_time_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create the index: 1.77 seconds\n"
     ]
    }
   ],
   "source": [
    "vector_storage,tokenizer,model,df_vector_storage_time_helper  = vector_db_creation(\"sentence-transformers/all-MiniLM-L6-v2\",r\"C:\\Users\\hongk\\Desktop\\embedding_test\\vector_database.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: Cardiac Catheterization\n",
      "Distances: [[55.445026 55.026802 54.568367 54.111465 53.71783 ]]\n",
      "Indices: [[129 316 449 126 237]]\n",
      "Nearest neighbors:\n",
      "Vaccination (Rabies)\n",
      "Coronary Artery Bypass Grafting (CABG)\n",
      "Pregnancy Test\n",
      "Vaccination (Shingles)\n",
      "MRI Scan\n"
     ]
    }
   ],
   "source": [
    "query_word = 'Cardiac Catheterization'\n",
    "query_vector = generate_embeddings([query_word], tokenizer, model)\n",
    "k = 5  # number of nearest neighbors to retrieve\n",
    "D, I = vector_storage.search(query_vector, k)\n",
    "# # Print results\n",
    "print(\"Query word:\", query_word)\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices:\", I)\n",
    "print(\"Nearest neighbors:\")\n",
    "for idx in I[0]:\n",
    "    print(df_list_of_documents['words'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: Cardiac Catheterization\n",
      "Distances: [[18.330544 18.217361 18.198412 18.198412 16.038517]]\n",
      "Indices: [[484 316 289  46 370]]\n",
      "Nearest neighbors:\n",
      "Cardiac Rehabilitation\n",
      "Coronary Artery Bypass Grafting (CABG)\n",
      "Cardiology Consultation\n",
      "Cardiology Consultation\n",
      "Bladder Sling Procedure\n"
     ]
    }
   ],
   "source": [
    "query_word = 'Cardiac Catheterization'\n",
    "query_vector = generate_embeddings([query_word], tokenizer, model)\n",
    "k = 5  # number of nearest neighbors to retrieve\n",
    "D, I = vector_storage.search(query_vector, k)\n",
    "# # Print results\n",
    "print(\"Query word:\", query_word)\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices:\", I)\n",
    "print(\"Nearest neighbors:\")\n",
    "for idx in I[0]:\n",
    "    print(df_list_of_documents['words'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: Cardiac Catheterization\n",
      "Distances: [[55.445026 55.026802 54.568367 54.111465 53.71783 ]]\n",
      "Indices: [[129 316 449 126 237]]\n",
      "Nearest neighbors:\n",
      "Vaccination (Rabies)\n",
      "Coronary Artery Bypass Grafting (CABG)\n",
      "Pregnancy Test\n",
      "Vaccination (Shingles)\n",
      "MRI Scan\n"
     ]
    }
   ],
   "source": [
    "for idx in I[0]:\n",
    "    print(df_list_of_documents['words'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_model = ['intfloat/e5-small-v2',\n",
    "                 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "                 ,'intfloat/multilingual-e5-large',\n",
    "                 'sentence-transformers/all-MiniLM-L12-v2',\n",
    "                 'google-bert/bert-base-uncased',\n",
    "                 'google-bert/bert-large-uncased',\n",
    "                 'allenai/longformer-base-4096',\n",
    "                 'jinaai/jina-embeddings-v2-base-en',\n",
    "                 #'google/bigbird-roberta-base',\n",
    "                 'pritamdeka/S-PubMedBert-MS-MARCO',\n",
    "                 'Charangan/MedBERT',\n",
    "                 'medicalai/ClinicalBERT',\n",
    "                 'dmis-lab/biobert-v1.1',\n",
    "                 'bvanaken/CORe-clinical-outcome-biobert-v1',\n",
    "                 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext',\n",
    "                 'yikuan8/Clinical-Longformer',\n",
    "                 #'yikuan8/Clinical-BigBird',\n",
    "                 #'Falconsai/medical_summarization',\n",
    "                 'UFNLP/gatortronS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vector_storage_time = pd.DataFrame(columns = ['model_name','total_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns = ['model_name','words','cosine_similarity','k','matched_words','time_taken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gout Treatment']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[df_list_of_documents['words'].tolist()[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intfloat/e5-small-v2',\n",
       " 'sentence-transformers/all-MiniLM-L6-v2',\n",
       " 'intfloat/multilingual-e5-large',\n",
       " 'sentence-transformers/all-MiniLM-L12-v2',\n",
       " 'google-bert/bert-base-uncased',\n",
       " 'google-bert/bert-large-uncased',\n",
       " 'allenai/longformer-base-4096',\n",
       " 'jinaai/jina-embeddings-v2-base-en',\n",
       " 'google/bigbird-roberta-base',\n",
       " 'pritamdeka/S-PubMedBert-MS-MARCO',\n",
       " 'Charangan/MedBERT',\n",
       " 'medicalai/ClinicalBERT',\n",
       " 'dmis-lab/biobert-v1.1',\n",
       " 'bvanaken/CORe-clinical-outcome-biobert-v1',\n",
       " 'microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext',\n",
       " 'yikuan8/Clinical-Longformer',\n",
       " 'yikuan8/Clinical-BigBird',\n",
       " 'Falconsai/medical_summarization',\n",
       " 'UFNLP/gatortronS']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigBirdModel(\n",
       "  (embeddings): BigBirdEmbeddings(\n",
       "    (word_embeddings): Embedding(50358, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(4096, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BigBirdEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BigBirdLayer(\n",
       "        (attention): BigBirdAttention(\n",
       "          (self): BigBirdSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BigBirdSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BigBirdIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): NewGELUActivation()\n",
       "        )\n",
       "        (output): BigBirdOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>3.035501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>1.928166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>3.262298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>2.997717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>1.823421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>3.063021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>3.062507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>1.983221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>3.234687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>1.737268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>3.013016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>3.219880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>1.798349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>3.014446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>2.930622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>1.756946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>intfloat/multilingual-e5-large</td>\n",
       "      <td>19.549787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L12-v2</td>\n",
       "      <td>3.072889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>google-bert/bert-base-uncased</td>\n",
       "      <td>6.330002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>google-bert/bert-large-uncased</td>\n",
       "      <td>18.912507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>allenai/longformer-base-4096</td>\n",
       "      <td>360.210631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>jinaai/jina-embeddings-v2-base-en</td>\n",
       "      <td>5.741727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model_name  total_time\n",
       "0                      intfloat/e5-small-v2    3.035501\n",
       "1    sentence-transformers/all-MiniLM-L6-v2    1.928166\n",
       "2                      intfloat/e5-small-v2    3.262298\n",
       "3                      intfloat/e5-small-v2    2.997717\n",
       "4    sentence-transformers/all-MiniLM-L6-v2    1.823421\n",
       "5                      intfloat/e5-small-v2    3.063021\n",
       "6                      intfloat/e5-small-v2    3.062507\n",
       "7    sentence-transformers/all-MiniLM-L6-v2    1.983221\n",
       "8                      intfloat/e5-small-v2    3.234687\n",
       "9    sentence-transformers/all-MiniLM-L6-v2    1.737268\n",
       "10                     intfloat/e5-small-v2    3.013016\n",
       "11                     intfloat/e5-small-v2    3.219880\n",
       "12   sentence-transformers/all-MiniLM-L6-v2    1.798349\n",
       "13                     intfloat/e5-small-v2    3.014446\n",
       "14                     intfloat/e5-small-v2    2.930622\n",
       "15   sentence-transformers/all-MiniLM-L6-v2    1.756946\n",
       "16           intfloat/multilingual-e5-large   19.549787\n",
       "17  sentence-transformers/all-MiniLM-L12-v2    3.072889\n",
       "18            google-bert/bert-base-uncased    6.330002\n",
       "19           google-bert/bert-large-uncased   18.912507\n",
       "20             allenai/longformer-base-4096  360.210631\n",
       "21        jinaai/jina-embeddings-v2-base-en    5.741727"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_vector_storage_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Falconsai/medical_summarization', 'UFNLP/gatortronS']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_model[17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vector_storage_time.to_csv(r\"C:\\Users\\hongk\\Desktop\\embedding_test\\df_vector_storage_timeV2.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hongk\\Desktop\\embedding_test\\.conda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hongk\\Desktop\\embedding_test\\.conda\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hongk\\.cache\\huggingface\\hub\\models--UFNLP--gatortronS. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create the index: 56.90 seconds\n",
      "StartUFNLP/gatortronS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hongk\\AppData\\Local\\Temp\\ipykernel_12668\\3733059328.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_results = pd.concat([df_results,df_results_helper])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EndUFNLP/gatortronS\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "list_of_words = df_testing['words'].tolist()\n",
    "list_of_words = [x for x in list_of_words if str(x) != 'nan']\n",
    "output_location = r\"C:\\Users\\hongk\\Desktop\\embedding_test\"\n",
    "for model_name in list_of_model[18:]:\n",
    "    vector_storage,tokenizer,model,df_vector_storage_time_helper  = vector_db_creation(model_name,r\"C:\\Users\\hongk\\Desktop\\embedding_test\\vector_database.xlsx\")\n",
    "    df_vector_storage_time = pd.concat([df_vector_storage_time,df_vector_storage_time_helper], ignore_index=True)\n",
    "    print('Start' + model_name)\n",
    "    df_results = pd.DataFrame(columns = ['model_name','words','cosine_similarity','k','matched_words','time_taken'])\n",
    "    for words in list_of_words:\n",
    "        start = time.time()\n",
    "        query = generate_embeddings([words], tokenizer, model)\n",
    "        D, I = vector_storage.search(query, 3)\n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "        for i, (distance, index) in enumerate(zip(D[0], I[0])):\n",
    "            df_results_helper = pd.DataFrame(columns = ['model_name','words','cosine_similarity','time_taken'])\n",
    "            df_results_helper['model_name'] = [model_name]\n",
    "            df_results_helper['words'] = [words]\n",
    "            df_results_helper['k'] = [i+1]\n",
    "            df_results_helper['matched_words'] = [df_list_of_documents['words'].tolist()[index]]\n",
    "            df_results_helper['cosine_similarity'] = [distance]\n",
    "            df_results_helper['time_taken'] = [total_time]\n",
    "            df_results = pd.concat([df_results,df_results_helper])\n",
    "    name = remove_words_before_slash(model_name)\n",
    "    df_results.to_excel(f\"{output_location}\\\\{name}_results.xlsx\",index = False)\n",
    "    print('End' + model_name)\n",
    "df_vector_storage_time.to_excel(f\"{output_location}\\\\vector_storage_time.xlsx\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>words</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>k</th>\n",
       "      <th>matched_words</th>\n",
       "      <th>time_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>A</td>\n",
       "      <td>39.541912</td>\n",
       "      <td>1</td>\n",
       "      <td>Vaccination (Rabies)</td>\n",
       "      <td>0.012002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>A</td>\n",
       "      <td>39.424778</td>\n",
       "      <td>2</td>\n",
       "      <td>Vaccination (Shingles)</td>\n",
       "      <td>0.012002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>A</td>\n",
       "      <td>38.481697</td>\n",
       "      <td>3</td>\n",
       "      <td>Copy of Medical Records</td>\n",
       "      <td>0.012002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>n</td>\n",
       "      <td>46.761600</td>\n",
       "      <td>1</td>\n",
       "      <td>D&amp;C (Dilation and Curettage)</td>\n",
       "      <td>0.012003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intfloat/e5-small-v2</td>\n",
       "      <td>n</td>\n",
       "      <td>46.688824</td>\n",
       "      <td>2</td>\n",
       "      <td>Vaccination (Rabies)</td>\n",
       "      <td>0.012003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>Cardiac Catheterization</td>\n",
       "      <td>18.217361</td>\n",
       "      <td>2</td>\n",
       "      <td>Coronary Artery Bypass Grafting (CABG)</td>\n",
       "      <td>0.008002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>Cardiac Catheterization</td>\n",
       "      <td>18.198412</td>\n",
       "      <td>3</td>\n",
       "      <td>Cardiology Consultation</td>\n",
       "      <td>0.008002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>Defibrillator Implantation</td>\n",
       "      <td>11.646187</td>\n",
       "      <td>1</td>\n",
       "      <td>Cochlear Implant Surgery</td>\n",
       "      <td>0.007002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>Defibrillator Implantation</td>\n",
       "      <td>9.677955</td>\n",
       "      <td>2</td>\n",
       "      <td>Mohs Surgery</td>\n",
       "      <td>0.007002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>Defibrillator Implantation</td>\n",
       "      <td>9.590363</td>\n",
       "      <td>3</td>\n",
       "      <td>Coronary Artery Bypass Grafting (CABG)</td>\n",
       "      <td>0.007002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                model_name                       words  \\\n",
       "0                     intfloat/e5-small-v2                           A   \n",
       "0                     intfloat/e5-small-v2                           A   \n",
       "0                     intfloat/e5-small-v2                           A   \n",
       "0                     intfloat/e5-small-v2                           n   \n",
       "0                     intfloat/e5-small-v2                           n   \n",
       "..                                     ...                         ...   \n",
       "0   sentence-transformers/all-MiniLM-L6-v2     Cardiac Catheterization   \n",
       "0   sentence-transformers/all-MiniLM-L6-v2     Cardiac Catheterization   \n",
       "0   sentence-transformers/all-MiniLM-L6-v2  Defibrillator Implantation   \n",
       "0   sentence-transformers/all-MiniLM-L6-v2  Defibrillator Implantation   \n",
       "0   sentence-transformers/all-MiniLM-L6-v2  Defibrillator Implantation   \n",
       "\n",
       "    cosine_similarity  k                           matched_words  time_taken  \n",
       "0           39.541912  1                    Vaccination (Rabies)    0.012002  \n",
       "0           39.424778  2                  Vaccination (Shingles)    0.012002  \n",
       "0           38.481697  3                 Copy of Medical Records    0.012002  \n",
       "0           46.761600  1            D&C (Dilation and Curettage)    0.012003  \n",
       "0           46.688824  2                    Vaccination (Rabies)    0.012003  \n",
       "..                ... ..                                     ...         ...  \n",
       "0           18.217361  2  Coronary Artery Bypass Grafting (CABG)    0.008002  \n",
       "0           18.198412  3                 Cardiology Consultation    0.008002  \n",
       "0           11.646187  1                Cochlear Implant Surgery    0.007002  \n",
       "0            9.677955  2                            Mohs Surgery    0.007002  \n",
       "0            9.590363  3  Coronary Artery Bypass Grafting (CABG)    0.007002  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>words</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>k</th>\n",
       "      <th>matched_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>Defibrillator Implantation</td>\n",
       "      <td>9.590363</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>3</td>\n",
       "      <td>Coronary Artery Bypass Grafting (CABG)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               model_name                       words  \\\n",
       "0  sentence-transformers/all-MiniLM-L6-v2  Defibrillator Implantation   \n",
       "\n",
       "   cosine_similarity  time_taken  k                           matched_words  \n",
       "0           9.590363    0.007002  3  Coronary Artery Bypass Grafting (CABG)  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jina-embeddings-v2-base-en\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = model_name+'_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.system(f'start excel.exe {name}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jinaai/jina-embeddings-v2-base-en_results'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[168], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[60], line 14\u001b[0m, in \u001b[0;36mgenerate_embeddings\u001b[1;34m(texts, tokenizer, model)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_embeddings\u001b[39m(texts, tokenizer, model):\n\u001b[1;32m---> 14\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     16\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[1;32mc:\\Users\\hongk\\Desktop\\embedding_test\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2883\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2881\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2882\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2883\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\hongk\\Desktop\\embedding_test\\.conda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2941\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[1;32m-> 2941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2944\u001b[0m     )\n\u001b[0;32m   2946\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[0;32m   2947\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2948\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2949\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2950\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "generate_embeddings([words], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hongk\\AppData\\Local\\Temp\\ipykernel_12668\\1611950516.py:1: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_vector_storage_time = pd.concat([df_vector_storage_time,df_vector_storage_time_helper], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_vector_storage_time = pd.concat([df_vector_storage_time,df_vector_storage_time_helper], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[215], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m query_vector \u001b[38;5;241m=\u001b[39m generate_embeddings([query_word], tokenizer, model)\n\u001b[0;32m      3\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# number of nearest neighbors to retrieve\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m D, I \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m(query_vector, k)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# # Print results\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery word:\u001b[39m\u001b[38;5;124m\"\u001b[39m, query_word)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "query_word = 'Cardiac Catheterization'\n",
    "query_vector = generate_embeddings([query_word], tokenizer, model)\n",
    "k = 5  # number of nearest neighbors to retrieve\n",
    "D, I = index.search(query_vector, k)\n",
    "# # Print results\n",
    "print(\"Query word:\", query_word)\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices:\", I)\n",
    "print(\"Nearest neighbors:\")\n",
    "for idx in I[0]:\n",
    "    print(df_list_of_documents['words'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar words to 'Cardiac Catheterization':\n",
      "1: Rehabilitation Program (distance: 564.20)\n",
      "2: Cardiac Rehabilitation (distance: 564.00)\n",
      "3: Temporal Arteritis Treatment (distance: 547.75)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m generate_embeddings([words], tokenizer, model)\n\u001b[0;32m      3\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m----> 4\u001b[0m D, I \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m(query, k)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 5 most similar words to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwords\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (distance, index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(D[\u001b[38;5;241m0\u001b[39m], I[\u001b[38;5;241m0\u001b[39m])):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'search'"
     ]
    }
   ],
   "source": [
    "for words in df_testing['words'].tolist()[0:5]:\n",
    "    query = generate_embeddings([words], tokenizer, model)\n",
    "    k = 3\n",
    "    D, I = index.search(query, k)\n",
    "    print(f\"Top 5 most similar words to '{words}':\")\n",
    "    for i, (distance, index) in enumerate(zip(D[0], I[0])):\n",
    "        print(f\"{i+1}: {df_list_of_documents['words'][index]} (distance: {distance:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>total_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jinaai/jina-embeddings-v2-base-en</td>\n",
       "      <td>5.201945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model_name  total_time\n",
       "0  jinaai/jina-embeddings-v2-base-en    5.201945"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: Cardiac Catheterization\n",
      "Distances: [[582.13074 576.70593 559.2464  549.9591  539.26886]]\n",
      "Indices: [[461 393 278 179  78]]\n",
      "Nearest neighbors:\n",
      "Mole Removal\n",
      "Breast Reconstruction\n",
      "Discharge Summary Fee\n",
      "Osteoarthritis Treatment\n",
      "Sports Medicine Consultation\n"
     ]
    }
   ],
   "source": [
    "query_word = 'Cardiac Catheterization'\n",
    "query_vector = generate_embeddings([query_word], tokenizer, model)\n",
    "k = 5  # number of nearest neighbors to retrieve\n",
    "D, I = index.search(query_vector, k)\n",
    "# # Print results\n",
    "print(\"Query word:\", query_word)\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices:\", I)\n",
    "print(\"Nearest neighbors:\")\n",
    "for idx in I[0]:\n",
    "    print(df_list_of_documents['words'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query word: washing clothes\n",
      "Distances: [[473.67096 458.19995 450.40442 448.18982 422.47223]]\n",
      "Indices: [[ 52  71  63  80 224]]\n",
      "Nearest neighbors:\n",
      " Nursing Services\n",
      " Specialist Services\n",
      " Dietary Services\n",
      " Outpatient Services\n",
      " Ear Cleaning\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Step 1: Load the Jina embeddings model\n",
    "model_name = 'jinaai/jina-embeddings-v2-base-en'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate embeddings\n",
    "def generate_embeddings(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Step 2: Read words from the CSV file\n",
    "csv_file = r\"C:\\Users\\hongk\\Desktop\\embedding_test\\embedding_trainign.xlsx\" # update with your CSV file path\n",
    "df_list_of_documents = pd.read_excel(csv_file)\n",
    "words = df_list_of_documents['words'].tolist()\n",
    "\n",
    "# # Step 3: Generate vectors for the words\n",
    "vectors = generate_embeddings(words, tokenizer, model)\n",
    "\n",
    "# # Step 4: Create the FAISS index\n",
    "dimension = vectors.shape[1]  # dimension of vectors (determined by the model)\n",
    "index = faiss.IndexFlatIP(dimension)  \n",
    "\n",
    "# # Step 5: Add vectors to the index\n",
    "index.add(vectors)\n",
    "\n",
    "# Step 6: Perform a search\n",
    "# For example, let's search for the nearest neighbors of the word 'example'\n",
    "query_word = 'washing clothes'\n",
    "query_vector = generate_embeddings([query_word], tokenizer, model)\n",
    "k = 5  # number of nearest neighbors to retrieve\n",
    "D, I = index.search(query_vector, k)\n",
    "\n",
    "# # Print results\n",
    "print(\"Query word:\", query_word)\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices:\", I)\n",
    "print(\"Nearest neighbors:\")\n",
    "for idx in I[0]:\n",
    "    print(df_list_of_documents['words'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hongk\\Desktop\\embedding_test\\.conda\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-06-13 10:39:54 | INFO | sentence_transformers.SentenceTransformer | Use pytorch device_name: cpu\n",
      "2024-06-13 10:39:54 | INFO | sentence_transformers.SentenceTransformer | Load pretrained SentenceTransformer: jinaai/jina-embeddings-v2-base-en\n",
      "c:\\Users\\hongk\\Desktop\\embedding_test\\.conda\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9341]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"jinaai/jina-embeddings-v2-base-en\", # switch to en/zh for English or Chinese\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# control your input sequence length up to 8192\n",
    "model.max_seq_length = 1024\n",
    "\n",
    "embeddings = model.encode([\n",
    "    'How is the weather today?',\n",
    "    'What is the current weather like today?'\n",
    "])\n",
    "print(cos_sim(embeddings[0], embeddings[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read excel sheet \n",
    "list_of_documents = pd.read_excel(r\"C:\\Users\\hongk\\Desktop\\embedding_test\\embedding_trainign.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Room Charge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Semi-private Room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>General Ward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intensive Care Unit (ICU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>High Dependency Unit (HDU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Copy of Medical Records</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>Insurance Pre-Authorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>Billing Inquiry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Patient Education Materials</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Health Coaching Session</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>293 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Room Charge\n",
       "0                    Private Room\n",
       "1               Semi-private Room\n",
       "2                    General Ward\n",
       "3       Intensive Care Unit (ICU)\n",
       "4      High Dependency Unit (HDU)\n",
       "..                            ...\n",
       "288       Copy of Medical Records\n",
       "289   Insurance Pre-Authorization\n",
       "290               Billing Inquiry\n",
       "291   Patient Education Materials\n",
       "292       Health Coaching Session\n",
       "\n",
       "[293 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = FAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1899,  1.1929, -0.4021,  ...,  0.6330, -0.5198, -0.8372],\n",
      "         [-0.2069,  1.3158, -0.3773,  ...,  0.6481, -0.3240, -0.9079],\n",
      "         [-0.1385,  1.3521, -0.2757,  ...,  0.7079, -0.5639, -0.9159],\n",
      "         ...,\n",
      "         [-0.2973,  1.0994, -0.3653,  ...,  0.6461, -0.1806, -0.7897],\n",
      "         [-0.1881,  1.0875, -0.2531,  ...,  0.6337, -0.1190, -0.9248],\n",
      "         [-0.1326,  1.0194, -0.1792,  ...,  0.5127, -0.3349, -0.9767]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"jinaai/jina-embeddings-v2-base-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to generate embeddings\n",
    "def get_embeddings(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fairseq\n",
      "  Using cached fairseq-0.12.2.tar.gz (9.6 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: cffi in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from fairseq) (1.16.0)\n",
      "Collecting cython (from fairseq)\n",
      "  Using cached Cython-3.0.10-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
      "  Using cached hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting omegaconf<2.1 (from fairseq)\n",
      "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from fairseq) (2023.10.3)\n",
      "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
      "  Using cached sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
      "Collecting torch (from fairseq)\n",
      "  Using cached torch-2.3.1-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from fairseq) (4.65.0)\n",
      "Requirement already satisfied: bitarray in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from fairseq) (2.9.2)\n",
      "Collecting torchaudio>=0.8.0 (from fairseq)\n",
      "  Using cached torchaudio-2.3.1-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from fairseq) (1.26.4)\n",
      "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
      "  Using cached antlr4_python3_runtime-4.8-py3-none-any.whl\n",
      "Requirement already satisfied: PyYAML>=5.1.* in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from omegaconf<2.1->fairseq) (4.9.0)\n",
      "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from sacrebleu>=1.4.12->fairseq) (4.9.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from torch->fairseq) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from torch->fairseq) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from torch->fairseq) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from torch->fairseq) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from torch->fairseq) (2023.10.0)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch->fairseq)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from cffi->fairseq) (2.21)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch->fairseq)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch->fairseq)\n",
      "  Using cached tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from jinja2->torch->fairseq) (2.1.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from portalocker->sacrebleu>=1.4.12->fairseq) (305.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hongk\\anaconda3\\lib\\site-packages (from sympy->torch->fairseq) (1.3.0)\n",
      "Using cached hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
      "Using cached omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
      "Using cached sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
      "Using cached torchaudio-2.3.1-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "Using cached torch-2.3.1-cp311-cp311-win_amd64.whl (159.8 MB)\n",
      "Using cached Cython-3.0.10-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Using cached tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: fairseq\n",
      "  Building wheel for fairseq (pyproject.toml): started\n",
      "  Building wheel for fairseq (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for fairseq: filename=fairseq-0.12.2-cp311-cp311-win_amd64.whl size=10349182 sha256=70ac68391f97bec226a2d182749fcf7a25a1eacc5341c91db6c75297acac7cfd\n",
      "  Stored in directory: c:\\users\\hongk\\appdata\\local\\pip\\cache\\wheels\\f1\\0d\\ed\\f4ca5b65eef7dda06cb421355a049648c032c6322c455a396e\n",
      "Successfully built fairseq\n",
      "Installing collected packages: tbb, intel-openmp, antlr4-python3-runtime, portalocker, omegaconf, mkl, cython, torch, sacrebleu, hydra-core, torchaudio, fairseq\n",
      "Successfully installed antlr4-python3-runtime-4.8 cython-3.0.10 fairseq-0.12.2 hydra-core-1.0.7 intel-openmp-2021.4.0 mkl-2021.4.0 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.2 tbb-2021.12.0 torch-2.3.1 torchaudio-2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "! pip install fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
